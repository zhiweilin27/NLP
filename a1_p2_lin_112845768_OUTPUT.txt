Checkpoint 2.1:

feature vector for the first 2 and last documents of the training set for word tokenizer is: 
 [[0. 0. 0. ... 0. 0. 1.]
 [1. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 0. 1.]]
feature vector for the first 2 and last documents of the training set for bpe tokenizer is: 
 [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]

Checkpoint 2.2:
 NOTE: betas for word tokenization models would be shown 

Model beta for receptor with a word tokenizer weight decay = 10 and dropout rate = 0 is: 
 [[ 0.01186802 -0.01741623  0.00556109 ... -0.01706391  0.01840947
   0.01041141]
 [ 0.00321183  0.014574    0.00405111 ... -0.00440618 -0.00112655
  -0.01180679]]
Model beta for reduction with a word tokenizer weight decay = 10 and dropout rate = 0 is: 
 [[ 0.0002047  -0.00943746 -0.01096988 ...  0.01188685  0.00580485
   0.04081355]
 [-0.00085653  0.01730049 -0.00190581 ... -0.01050256 -0.00593274
  -0.00445091]
 [-0.01670209 -0.00509073  0.00571162 ...  0.01383479 -0.00219197
  -0.05325593]]
Model beta for rate with a word tokenizer weight decay = 10 and dropout rate = 0 is: 
 [[ 0.01411528 -0.0064601  -0.01546413 ... -0.01190394  0.00675071
   0.02188052]
 [ 0.01062183 -0.00154536  0.01171054 ... -0.00754149  0.00920694
  -0.000518  ]
 [-0.00322944 -0.00524973  0.00548329 ... -0.01469926 -0.01120952
  -0.03468653]
 [-0.00699892 -0.01570908  0.0087563  ...  0.01332961  0.01613995
   0.00771713]]
Model beta for reserve with a word tokenizer weight decay = 10 and dropout rate = 0 is: 
 [[-0.00660889  0.00452392 -0.01580295 ...  0.01840049  0.01847276
  -0.00080786]
 [-0.00287076  0.00581373  0.00828037 ... -0.00705563  0.01453029
  -0.00470184]
 [ 0.01251176 -0.00775998 -0.01658505 ...  0.00456499 -0.01660028
   0.02311574]
 [-0.00095137  0.00941208 -0.00109688 ... -0.01593748  0.01394416
  -0.01454826]
 [-0.01409124  0.01385073  0.01829115 ... -0.01461421 -0.01777144
  -0.00051423]]
Model beta for reason with a word tokenizer weight decay = 10 and dropout rate = 0 is: 
 [[ 0.00277063 -0.01186409 -0.01088363 ... -0.01226069  0.01347577
  -0.03046039]
 [-0.00785014  0.00199906 -0.00157144 ...  0.01062579 -0.00602592
   0.02551853]
 [-0.01913085 -0.01847631 -0.01364977 ...  0.01451876  0.00157014
  -0.01364726]
 [-0.01131558 -0.0051614   0.00433074 ... -0.01739278  0.01483362
   0.02286529]
 [ 0.01759062 -0.00060033 -0.00243521 ...  0.00639415  0.01294855
   0.01287056]
 [ 0.00511079  0.00982818 -0.00031559 ...  0.00077432  0.01254729
  -0.02535662]]
Model beta for return with a word tokenizer weight decay = 10 and dropout rate = 0 is: 
 [[-0.01035676 -0.01044964 -0.01126129 ...  0.00824205  0.01748397
   0.0364849 ]
 [-0.0055731  -0.00405763 -0.01621415 ...  0.01836121  0.00175848
   0.05655436]
 [ 0.01337417  0.00726976 -0.0073788  ...  0.00500823 -0.01852728
  -0.01678522]
 ...
 [-0.00434282 -0.01445738  0.00814182 ... -0.01039292  0.01039385
   0.00758087]
 [-0.00575402  0.01639732 -0.01866048 ... -0.01385033  0.01214258
  -0.00925922]
 [ 0.0051057   0.01819408 -0.01156527 ...  0.01249993  0.00396686
  -0.01728286]]

Checkpoint 2.3:
 Note: F1 values for both types of tokenization models will be shown with weight decay = 10 and dropout rate = 0

Average F1 score across 6 words with word tokenizer weight decay = 10 and dropout rate = 0 is: 0.19052091442435815.
Average F1 score across 6 words with bpe tokenizer weight decay = 10 and dropout rate = 0 is: 0.21645337992691074.

Checkpoint 2.4:

+---------+------+-------+-------+-------+
|    word |    0 |   0.1 |   0.2 |   0.5 |
+=========+======+=======+=======+=======+
|   0.001 | 0.42 |  0.43 |  0.42 |  0.43 |
+---------+------+-------+-------+-------+
|   0.01  | 0.41 |  0.42 |  0.42 |  0.43 |
+---------+------+-------+-------+-------+
|   0.1   | 0.41 |  0.41 |  0.42 |  0.39 |
+---------+------+-------+-------+-------+
|   1     | 0.33 |  0.34 |  0.33 |  0.31 |
+---------+------+-------+-------+-------+
|  10     | 0.19 |  0.19 |  0.19 |  0.18 |
+---------+------+-------+-------+-------+
| 100     | 0.19 |  0.19 |  0.17 |  0.19 |
+---------+------+-------+-------+-------+
+---------+------+-------+-------+-------+
|     bpe |    0 |   0.1 |   0.2 |   0.5 |
+=========+======+=======+=======+=======+
|   0.001 | 0.51 |  0.51 |  0.52 |  0.53 |
+---------+------+-------+-------+-------+
|   0.01  | 0.51 |  0.51 |  0.51 |  0.52 |
+---------+------+-------+-------+-------+
|   0.1   | 0.46 |  0.46 |  0.46 |  0.49 |
+---------+------+-------+-------+-------+
|   1     | 0.34 |  0.34 |  0.34 |  0.34 |
+---------+------+-------+-------+-------+
|  10     | 0.22 |  0.25 |  0.28 |  0.18 |
+---------+------+-------+-------+-------+
| 100     | 0.23 |  0.23 |  0.22 |  0.17 |
+---------+------+-------+-------+-------+
The best average F1 score across 6 words with a set of weight decay = 0.001,dropout rate = 0.5, and tokenizer = bpe is: 0.530320533914465 

Checkpoint 2.5:

f1 scores for each word with a set of weight decay=0.01, dropout_rate=0.1, tokenizer=bpe, lr=0.1 and epochs=70, momentum=0.999, Nestero=True in 2.5 is:
 +-----------+----------------+----------------+------------+
| Word      |   Weight Decay |   Dropout Rate |   F1 Score |
+===========+================+================+============+
| receptor  |           0.01 |            0.1 |  0.754831  |
+-----------+----------------+----------------+------------+
| reduction |           0.01 |            0.1 |  0.637015  |
+-----------+----------------+----------------+------------+
| rate      |           0.01 |            0.1 |  0.708946  |
+-----------+----------------+----------------+------------+
| reserve   |           0.01 |            0.1 |  0.914872  |
+-----------+----------------+----------------+------------+
| reason    |           0.01 |            0.1 |  0.311197  |
+-----------+----------------+----------------+------------+
| return    |           0.01 |            0.1 |  0.0518116 |
+-----------+----------------+----------------+------------+
Average of best f1 scores across 6 target words is : 0.5631119965058868

Total time taken for entire part 2: 280.84971499443054 

For 2.5, I used Nesterov momentum factor,0.999 and Nesterov=True, in the learning rate = 0.1 in SGD and changed the max epoch to 70.
The reason for these changes is that I believe the original SGD converges too slowly for this logistic regression. So, using Nesterov momentum to accelerate the convergence, and also training the model longer with more epochs allows the model to learn more details in the train set, which means it gets closer to the minimum point
